function net = cnn_mnist_init_mod(varargin)
% CNN_MNIST_INIT_MOD Initialize various small NNs for MNIST

% Modified from original matconvnet distribution: cnn_mnist_init.m
% TO-DO: merge/replace main cnn_mnist_init.m

opts.batchNormalization = true;
opts.networkType = 'simplenn';
opts.modelType = 'simple'; % 'lenet_base';

% opts.train = struct();
opts.learningRate = 0.001;
opts.numEpochs = 20;
opts.batchSize = 100;

opts.useWeightNorm = false;

opts = vl_argparse(opts, varargin);

rng('default');
rng(0);
rng('shuffle')

f = 1/100;
net.layers = {};

% net.layers{end+1} = struct('type', 'conv', ...
%                            'weights', {{f*randn(5, 5, 1, 20, 'single'), zeros(1, 20, 'single')}}, ...
%                            'stride', 1, ...
%                            'pad', 0) ;
% net.layers{end+1} = struct('type', 'pool', ...
%                            'method', 'max', ...
%                            'pool', [2 2], ...
%                            'stride', 2, ...
%                            'pad', 0) ;
% net.layers{end+1} = struct('type', 'conv', ...
%                            'weights', {{f*randn(12, 12, 20, 10, 'single'), zeros(1, 10, 'single')}}, ...
%                            'stride', 1, ...
%                            'pad', 0) ;
% net.layers{end+1} = struct('type', 'loss') ;


switch opts.modelType
    
    case 'cnn_mnist'
        % matconvnet default for MNIST examples
        
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(5,5,1,20, 'single'), zeros(1, 20, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(5,5,20,50, 'single'), zeros(1,50,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(4,4,50,500, 'single'), zeros(1,500,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'relu') ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(1,1,500,10, 'single'), zeros(1,10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;             
        
%     case 'cnn_2_layer'
%         % simple net used in regularization development exp
%         
%         net.layers{end+1} = struct('type', 'conv', ...
%             'weights', {{f*randn(5, 5, 1, 20, 'single'), zeros(1, 20, 'single')}}, ...
%             'stride', 1, ...
%             'pad', 0) ;
%         net.layers{end+1} = struct('type', 'pool', ...
%             'method', 'max', ...
%             'pool', [2 2], ...
%             'stride', 2, ...
%             'pad', 0) ;
%         net.layers{end+1} = struct('type', 'conv', ...
%             'weights', {{f*randn(12, 12, 20, 20, 'single'), zeros(1, 20, 'single')}}, ...
%             'stride', 1, ...
%             'pad', 0) ;
%         net.layers{end+1} = struct('type', 'relu') ;
%         net.layers{end+1} = struct('type', 'conv', ...
%             'weights', {{f*randn(1, 1, 20, 10, 'single'), zeros(1, 10,'single')}}, ...
%             'stride', 1, ...
%             'pad', 0) ;
%         net.layers{end+1} = struct('type', 'loss') ;
        
        
    case 'cnn_1_layer'
        % basic 1 layer CNN
        
        nFilters = 25; % 5
        nL1 = 5; nL2 = 12; % receptive field sizes
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL1, nL1, 1, nFilters, 'single'), zeros(1, nFilters, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL2, nL2, nFilters, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;
        
   case 'cnn_1_layer_mpool'
        % basic 1 layer CNN with max pooling over groups of neurons (ala
        % maxout)
        
        nFilters = 25; % 5
        nL1 = 5; nL2 = 12; % receptive field sizes
        groupSize = 5;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL1, nL1, 1, nFilters, 'single'), zeros(1, nFilters, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'maxout', ...
            'groupSize', groupSize, ...
            'method', 'max');
            %'pool', [2 2], ...
            %'stride', 2, ...
            %'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL2, nL2, nFilters/groupSize, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;         
        
        
    case 'cnn_1_layer_11'
        % basic 1 layer CNN
        
        nFilters = 25; % 5
        nL1 = 11; nL2 = 9; % receptive field sizes
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{vl_nnwinit(nL1, nL1, 1, nFilters, 'initrandn', f), zeros(1, nFilters, 'single')}}, ...                        
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL2, nL2, nFilters, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;
        
        
    case 'cnn_1_layer_15'
        % basic 1 layer CNN
        
        nFilters = 25; % 5
        nL1 = 15; nL2 = 7; % receptive field sizes
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL1, nL1, 1, nFilters, 'single'), zeros(1, nFilters, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL2, nL2, nFilters, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;
        
            
     case 'cnn_1_layer_11_mpool'
        % basic 1 layer CNN with max pooling over groups of neurons (ala
        % maxout)
        
        nFilters = 25; % 5
        nL1 = 11; nL2 = 9; % receptive field sizes
        groupSize = 5;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL1, nL1, 1, nFilters, 'single'), zeros(1, nFilters, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'maxout', ...
            'groupSize', groupSize, ...
            'method', 'pmo');
            %'pool', [2 2], ...
            %'stride', 2, ...
            %'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL2, nL2, nFilters/groupSize, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;    
        
        
    case 'cnn_2_layer'
        
        nF1 = 25; nF2 = 500;  % number of filters
        nL1 = 5; nL2 = 12;   % receptive field sizes
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL1, nL1, 1, nF1, 'single'), zeros(1, nF1, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'pool', ...
            'method', 'max', ...
            'pool', [2 2], ...
            'stride', 2, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(nL2, nL2, nF1, nF2, 'single'), zeros(1, nF2, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'relu') ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(1, 1, nF2, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss') ;
        
        
    case 'dnn_1_layer'
        % 1 hidden layer/perceptron
        
        nFilters = 25; %5
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{vl_nnwinit(28, 28, 1, nFilters, 'initrelu', f), zeros(1, nFilters, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'relu');
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{vl_nnwinit(1, 1, nFilters, 10, 'initvar', f), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss');
        
   case 'dnn_2_layer'
        % 1 hidden layer/perceptron
        
        nF1 = 20; %5
        nF2 = 20;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(28, 28, 1, nF1, 'single'), zeros(1, nF1, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'relu') ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(1, 1, nF1, nF2, 'single'), zeros(1, nF2, 'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'relu') ;
        net.layers{end+1} = struct('type', 'conv', ...
            'weights', {{f*randn(1, 1, nF2, 10, 'single'), zeros(1, 10,'single')}}, ...
            'stride', 1, ...
            'pad', 0) ;
        net.layers{end+1} = struct('type', 'loss');    
        
end


% optionally switch to batch normalization
if opts.batchNormalization
    nLayers = numel(net.layers);
    l = 1;
    while l <= nLayers - 2 % look for conv layers to all but classifier
        if strcmp(net.layers{l}.type, 'conv')
            net = insertBnorm(net, l);
            l = l + 1;
            nLayers = nLayers + 1;
        end
        l = l + 1;
    end
end

% optionally normalize the initial convolutional weights
if opts.useWeightNorm
    net = normalizeConvWeights(net);     
end

% Meta parameters
net.meta.inputSize = [28 28 1] ;
net.meta.trainOpts.learningRate = opts.learningRate;
net.meta.trainOpts.numEpochs = opts.numEpochs;
net.meta.trainOpts.batchSize = opts.batchSize;

% Fill in default values
net = vl_simplenn_tidy(net) ;

% Switch to DagNN if requested
switch lower(opts.networkType)
    case 'simplenn'
        % done
    case 'dagnn'
        net = dagnn.DagNN.fromSimpleNN(net, 'canonicalNames', true) ;
        net.addLayer('error', dagnn.Loss('loss', 'classerror'), ...
            {'prediction','label'}, 'error') ;
    otherwise
        assert(false) ;
end

% --------------------------------------------------------------------
function net = insertBnorm(net, l)
% --------------------------------------------------------------------
assert(isfield(net.layers{l}, 'weights'));
ndim = size(net.layers{l}.weights{1}, 4);
layer = struct('type', 'bnorm', ...
    'weights', {{ones(ndim, 1, 'single'), zeros(ndim, 1, 'single')}}, ...
    'learningRate', [1 1 0.05], ...
    'weightDecay', [0 0]) ;
net.layers{l}.biases = [] ;
net.layers = horzcat(net.layers(1:l), layer, net.layers(l+1:end)) ;

% --------------------------------------------------------------------
function net = normalizeConvWeights(net)
% --------------------------------------------------------------------
% weight normalization/projected gradient descend

for l=numel(net.layers)-2:-1:1 % all exept the classifier layer
    if strcmp(net.layers{l}.type, 'conv')
        W = net.layers{l}.weights{1};
        net.layers{l}.weights{1} = bsxfun(@rdivide, W, sqrt(sum(sum(W.^2))));
    end
end
